# train.py

import os
import sys
import time
import numpy as np
import torch
from torch import nn
from torch.utils.data import DataLoader
import torch.nn.functional as F
from tqdm import tqdm
import logging

from dataset import AMOS_Dataset
from models.model import DIF_Net
from utils import convert_cuda, save_visualization_3view, simple_eval, gpu_slice_volume, GPUDailyScanSimulator, ElasticDeformation, simple_eval_metric, compute_gradient



# ==========================================
#  é…ç½®åŒºåŸŸ
# ==========================================
class Config:
    name = 'dif_amos_roi_v2' # å»ºè®®æ”¹åä»¥åŒºåˆ†æ—§å®éªŒ
    # æŒ‡å‘ä½ åˆšæ‰é¢„å¤„ç†åçš„æ•°æ®ç›˜è·¯å¾„
    data_root = r'/root/autodl-tmp/Proj/data/amos_mri_npy'
    # æ–°å¢ï¼šæŒ‡å‘ä½ ç”Ÿæˆçš„ ROI JSON æ–‡ä»¶å¤¹
    label_root = r'/root/autodl-tmp/Proj/data/amos_mri_label_npy'

    gpu_id = 0
    num_workers = 22 # é…åˆæ•°æ®ç›˜è¯»å–ï¼Œä¸éœ€è¦è®¾ç½®è¿‡å¤§
    preload = False # å¦‚æœå†…å­˜ä¸å¤Ÿï¼ˆç³»ç»Ÿç›˜çˆ†è¿‡ï¼‰ï¼Œå»ºè®®è®¾ä¸º False
    batch_size = 1
    epoch = 400
    lr = 1e-3
    num_views = 3
    out_res = (256, 256, 128)
    num_points = 100000 # é…åˆ ROI é‡‡æ ·ï¼Œ10w ç‚¹å°±èƒ½è¾¾åˆ°å¾ˆå¥½çš„æ•ˆæœ
    combine = 'attention'
    eval_freq = 10
    save_freq = 50
    gamma = 0.95
    sigma = (0.02, 0.02, 0.08)


def worker_init_fn(worker_id):
    np.random.seed((worker_id + torch.initial_seed()) % np.iinfo(np.int32).max)

def setup_logger(log_file):
    logger = logging.getLogger()
    logger.setLevel(logging.INFO)

    # æ–‡ä»¶è¾“å‡º
    fh = logging.FileHandler(log_file, mode='a')
    fh.setFormatter(logging.Formatter('%(asctime)s - %(message)s'))
    logger.addHandler(fh)

    # æ§åˆ¶å°è¾“å‡º
    ch = logging.StreamHandler()
    ch.setFormatter(logging.Formatter('%(message)s'))
    logger.addHandler(ch)

    return logger

if __name__ == '__main__':
    os.environ['CUDA_VISIBLE_DEVICES'] = str(Config.gpu_id)
    save_dir = f'./logs/{Config.name}'
    os.makedirs(save_dir, exist_ok=True)

    logger = setup_logger(os.path.join(save_dir, 'train_log.txt'))
    logger.info(f"Start training: {Config.name}")
    logger.info(f"Config: Batch={Config.batch_size}, LR={Config.lr}, Sigma={Config.sigma}")

    train_dst = AMOS_Dataset(
        data_root=Config.data_root,
        label_root=Config.label_root,
        split='train',
        npoint=Config.num_points,
        out_res=Config.out_res
    )
    # å¦‚æœ preload=Falseï¼Œå»ºè®®è®¾ç½® num_workers å¼€å¯å¤šçº¿ç¨‹è¯»å–
    train_loader = DataLoader(
        train_dst,
        batch_size=Config.batch_size,
        shuffle=True,
        num_workers=4,
        pin_memory=True,
        worker_init_fn=worker_init_fn
    )

    val_dst = AMOS_Dataset(
        data_root=Config.data_root,
        label_root=Config.label_root,
        split='eval',
        npoint=50000, # è¯„ä¼°æ—¶é‡‡æ ·ç‚¹å¯ä»¥å°‘ä¸€ç‚¹
        out_res=Config.out_res
    )

    eval_loader = DataLoader(val_dst, batch_size=1, shuffle=False)

    # Model
    model = DIF_Net(num_views=Config.num_views, combine=Config.combine).cuda()

    # Simulator (Data Augmentation)
    # å™ªå£°ç”Ÿæˆå™¨
    train_simulator = GPUDailyScanSimulator(
        noise_level=0.0,
        blur_sigma=0.0
    ).cuda()

    eval_simulator = GPUDailyScanSimulator(
        noise_level=0.0,
        blur_sigma=0.0
    ).cuda()

    deformer = ElasticDeformation(grid_size=8, sigma=Config.sigma).cuda()

    optimizer = torch.optim.Adam(model.parameters(), lr=Config.lr, weight_decay=0)
    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=Config.gamma)

    logger.info("Start Training Loop...")
    epoch = 0
    while epoch <= Config.epoch:
        loss_list = []
        model.train()

        with tqdm(train_loader, desc=f'Epoch {epoch}/{Config.epoch}', ncols=120, unit='img') as pbar:
            for item in pbar:
                optimizer.zero_grad()
                item = convert_cuda(item)

                with torch.no_grad():
                    # ğŸŸ¢ é€»è¾‘ç¿»è½¬ï¼šæºå›¾åƒä½œä¸º Priorï¼Œå½¢å˜äº§ç”Ÿ Target
                    prior_vol = item['image']
                    target_vol = deformer(prior_vol, mode='bilinear')

                    # ğŸŸ¢ ä» Target ä¸­åˆ‡ç‰‡å¹¶æ›´æ–°è¾“å…¥
                    item['projs'] = gpu_slice_volume(target_vol)
                    item['prior'] = prior_vol

                    # ğŸŸ¢ åŠ¨æ€é‡æ–°é‡‡æ · GTï¼Œå› ä¸ºçœŸå®çš„ç‰©ç†çŠ¶æ€ (Target) å·²ç»æ”¹å˜
                    uv = item['points']
                    uv_sampling = uv[..., [2, 1, 0]].reshape(uv.shape[0], 1, 1, uv.shape[1], 3)
                    gt = F.grid_sample(target_vol, uv_sampling, align_corners=True)[:, :, 0, 0, :]
                    item['p_gt'] = gt

                pred_val, delta_coords = model(item)

                # å– Loss æœ€å¤§çš„å‰ hard_ratioçš„ç‚¹
                loss_pixel = F.l1_loss(pred_val, gt, reduction='none')
                loss_flat = loss_pixel.view(-1)
                hard_ratio = 1
                k = int(loss_flat.numel() * hard_ratio)
                topk_loss, _ = torch.topk(loss_flat, k)
                loss_recon = topk_loss.mean()

                loss_reg = torch.mean(delta_coords ** 2)

                # æ€» Loss
                w_reg = 0.02
                loss = loss_recon + w_reg * loss_reg
                loss_list.append(loss.item())
                loss.backward()

                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                optimizer.step()

                current_lr = optimizer.param_groups[0]["lr"]
                pbar.set_postfix({'loss': f'{np.mean(loss_list):.6f}', 'lr': f'{current_lr:.6f}'})

        logger.info(f"Epoch {epoch} | Train Loss: {np.mean(loss_list):.6f} | LR: {current_lr:.2e}")


        if epoch > 0 and epoch % Config.save_freq == 0:
            torch.save(model.state_dict(), os.path.join(save_dir, f'ep_{epoch}.pth'))

        if epoch == 0 or epoch % Config.eval_freq == 0:
            print(f" --> Running Evaluation at Epoch {epoch}...")

            eval_start_time = time.time()

            save_visualization_3view(
                model, val_dst, epoch,
                save_dir=os.path.join(save_dir, 'vis'),
                simulator=eval_simulator,
                prior_deformer=deformer  # ä¼ å…¥ä¿®æ”¹åçš„å½¢å˜å™¨
            )

            model.eval()
            psnrs, ssims = [], []
            inference_times = []

            with torch.no_grad():
                for i, v_item in enumerate(eval_loader):
                    if i>=5: break
                    v_item = convert_cuda(v_item)

                    # ğŸŸ¢ è¯„ä¼°é˜¶æ®µé€»è¾‘ç¿»è½¬
                    prior_vol = v_item['image']
                    target_vol = deformer(prior_vol, mode='bilinear')

                    v_item['projs'] = gpu_slice_volume(target_vol)
                    v_item['prior'] = prior_vol

                    torch.cuda.synchronize()
                    t_start = time.time()

                    pred, _ = model(v_item, is_eval=True, eval_npoint=50000)

                    torch.cuda.synchronize()
                    t_end = time.time()
                    inference_times.append(t_end - t_start)

                    pred = pred[0, 0].cpu().numpy().reshape(v_item['image'].shape[2:])
                    # ğŸŸ¢ SSIM çš„åŸºå‡†å¯¹è±¡å¿…é¡»æ˜¯ Target
                    gt_img = target_vol.cpu().numpy()[0, 0]

                    # èƒŒæ™¯æ»¤é™¤
                    # pred[pred < 0.05] = 0

                    p, s = simple_eval_metric(gt_img, pred)
                    psnrs.append(p)
                    ssims.append(s)


            avg_psnr = np.mean(psnrs)
            avg_ssim = np.mean(ssims)
            avg_time = np.mean(inference_times)
            eval_msg = f" Â  Â  [Eval Result] Epoch {epoch}: PSNR = {avg_psnr:.4f} | SSIM = {avg_ssim:.4f}"
            print(f"Inference Time = {avg_time:.4f}s")
            logger.info(eval_msg)

        lr_scheduler.step()
        epoch += 1

