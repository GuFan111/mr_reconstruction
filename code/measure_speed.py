import torch
import torch.nn.functional as F
import time
import numpy as np
import scipy.ndimage as ndimage
from monai.inferers import sliding_window_inference
from models.mednext.create_mednext_v1 import create_mednext_v1

# ================= é…ç½®åŒºåŸŸ =================
class Config:
    # æ¨¡æ‹Ÿè¾“å…¥æ•°æ®çš„å½¢çŠ¶ [Batch, Channel, D, H, W] (AMOS MRI å…¸å‹å°ºå¯¸)
    input_shape = (1, 1, 100, 256, 256) 
    
    num_classes = 16
    
    # è®­ç»ƒæ—¶çš„ Patch Size (æ³¨æ„é¡ºåº: X, Y, Z)
    # ç¡®ä¿ fast_inference çš„ input_size ä¸æ­¤ä¸€è‡´
    crop_size = (160, 160, 96) 
    
    overlap = 0.1               # æ»‘åŠ¨çª—å£é‡å ç‡
    sw_batch_size = 4           # æ»‘åŠ¨çª—å£ Batch Size
    model_id = 'S'              # Model Size

# ================= ğŸŒ æ—§æ–¹æ¡ˆ: CPU åå¤„ç† (æ…¢) =================
def cpu_post_process(pred_tensor):
    """
    ä¼ ç»Ÿçš„ CPU åå¤„ç†ç®¡é“
    """
    # 1. GPU -> CPU (è€—æ—¶!)
    pred_np = torch.argmax(pred_tensor, dim=1).cpu().numpy()[0]
    
    # 2. Scipy è¿é€šåŸŸ/é—­è¿ç®— (CPUè®¡ç®—, æ…¢!)
    # æ¨¡æ‹Ÿä¸€ä¸ªç®€å•çš„é—­è¿ç®—
    struct = ndimage.generate_binary_structure(3, 1)
    pred_np = ndimage.binary_closing(pred_np, structure=struct, iterations=1)
    
    return pred_np

# ================= ğŸš€ æ–°æ–¹æ¡ˆ: çº¯ GPU ç®¡é“ (å¿«) =================
class FastROIPipeline:
    def __init__(self, model, input_size=(160, 160, 96)):
        self.model = model
        self.input_size = input_size
        
    def gpu_morphology_closing(self, mask_tensor, kernel_size=5):
        """
        åœ¨ GPU ä¸Šæ‰§è¡Œå½¢æ€å­¦é—­è¿ç®— (å¡«è¡¥æ–­è£‚)
        è¾“å…¥: [B, 1, D, H, W] çš„ 0/1 Float Tensor
        """
        # Padding è®¡ç®—: kernel_size // 2
        pad = kernel_size // 2
        
        # 1. è†¨èƒ€ (Dilation) - è¿æ¥æ–­è£‚
        # MaxPool3d ç›¸å½“äºè†¨èƒ€
        dilated = F.max_pool3d(mask_tensor, kernel_size=kernel_size, stride=1, padding=pad)
        
        # 2. è…èš€ (Erosion) - æ¢å¤åŸå¤§å° 
        # -MaxPool3d(-x) ç›¸å½“äºè…èš€
        closed = -F.max_pool3d(-dilated, kernel_size=kernel_size, stride=1, padding=pad)
        
        return closed

    def run(self, inputs):
        """
        å…¨æµç¨‹ GPU æ¨ç† (ä¸å›ä¼  CPU)
        """
        original_size = inputs.shape[2:]
        
        # 1. é™é‡‡æ · (GPU)
        inputs_small = F.interpolate(inputs, size=self.input_size, mode='area')
        
        # 2. æ¨¡å‹æ¨ç† (GPU + FP16)
        with torch.cuda.amp.autocast():
            with torch.no_grad():
                logits_small = self.model(inputs_small)
        
        # 3. ä¸Šé‡‡æ · (GPU)
        logits_large = F.interpolate(logits_small, size=original_size, mode='trilinear', align_corners=False)
        
        # 4. ç”Ÿæˆ Mask (GPU)
        pred_mask = torch.argmax(logits_large, dim=1, keepdim=True).float()
        
        # 5. å½¢æ€å­¦åå¤„ç† (GPU)
        final_mask = self.gpu_morphology_closing(pred_mask, kernel_size=5)
        
        return final_mask

# ================= ä¸»å‡½æ•° =================
def main():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Running speed test on device: {device}")
    print(f"Input Shape: {Config.input_shape}")
    
    # 1. åˆå§‹åŒ–æ¨¡å‹
    model = create_mednext_v1(
        num_input_channels=1,
        num_classes=Config.num_classes,
        model_id=Config.model_id,
        kernel_size=3,
        deep_supervision=False
    ).to(device)
    model.eval()

    # 2. å‡†å¤‡æ•°æ®
    dummy_input = torch.randn(Config.input_shape).to(device)
    
    # 3. åˆå§‹åŒ–å¿«é€Ÿç®¡é“
    fast_pipeline = FastROIPipeline(model, input_size=Config.crop_size)

    # ---------------------------------------------------------
    # é˜¶æ®µ 1: é¢„çƒ­ (Warm-up)
    # ---------------------------------------------------------
    print("\n[Phase 1] Warming up GPU...")
    with torch.no_grad():
        for _ in range(10):
            _ = fast_pipeline.run(dummy_input)
    torch.cuda.synchronize()

    # ---------------------------------------------------------
    # é˜¶æ®µ 2: æµ‹è¯•åŸºå‡† (Sliding Window + CPU Post-Process)
    # ---------------------------------------------------------
    print(f"\nğŸŒ [Baseline] Sliding Window + CPU Post-Process:")
    print(f"   (This simulates your OLD method)")
    
    start_time = time.time()
    loops_slow = 3 # è·‘æ…¢ç‚¹ï¼Œè·‘å¤šäº†æµªè´¹æ—¶é—´
    
    with torch.no_grad():
        for _ in range(loops_slow):
            # A. æ¨ç†
            output = sliding_window_inference(
                dummy_input, roi_size=Config.crop_size, 
                sw_batch_size=Config.sw_batch_size, predictor=model, overlap=Config.overlap
            )
            # B. CPU åå¤„ç†
            _ = cpu_post_process(output)
            
    torch.cuda.synchronize() # ç¡®ä¿ CPU ä»»åŠ¡ä¹Ÿå®Œæˆäº†
    end_time = time.time()
    
    avg_slow = (end_time - start_time) / loops_slow
    print(f"   â±ï¸  Average Time: {avg_slow:.4f} s ({avg_slow*1000:.1f} ms)")

    # ---------------------------------------------------------
    # é˜¶æ®µ 3: æµ‹è¯•ä¼˜åŒ–æ–¹æ¡ˆ (Global Resize + GPU Post-Process)
    # ---------------------------------------------------------
    print(f"\nğŸš€ [Optimized] Global Resize + GPU Post-Process:")
    print(f"   (This simulates your NEW method for MR-Linac)")
    
    start_time = time.time()
    loops_fast = 100 # é€Ÿåº¦å¿«ï¼Œå¤šè·‘ç‚¹æ±‚å¹³å‡
    
    with torch.no_grad():
        for _ in range(loops_fast):
            # å…¨æµç¨‹éƒ½åœ¨ GPU ä¸Š
            _ = fast_pipeline.run(dummy_input)
            
    torch.cuda.synchronize()
    end_time = time.time()
    
    avg_fast = (end_time - start_time) / loops_fast
    print(f"   â±ï¸  Average Time: {avg_fast:.4f} s ({avg_fast*1000:.1f} ms)")
    print(f"   ğŸ”¥ FPS: {1/avg_fast:.1f}")

    # ---------------------------------------------------------
    # æ€»ç»“
    # ---------------------------------------------------------
    print("-" * 40)
    print(f"ğŸ“Š Speedup Factor: {avg_slow / avg_fast:.1f}x Faster")
    if avg_fast < 0.05:
        print("âœ… Status: Real-time Requirement (<50ms) MET!")
    else:
        print("âš ï¸ Status: Still optimization needed.")

if __name__ == '__main__':
    main()